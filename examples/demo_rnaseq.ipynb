{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEDD for Masked RNA-seq Prediction\n",
    "\n",
    "This notebook demonstrates how to use Score-Entropy Discrete Diffusion (SEDD) for masked gene expression prediction on single-cell RNA-seq data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "SEDD is a discrete diffusion model that learns to predict masked tokens. For RNA-seq data:\n",
    "- Gene expression values are discretized into bins\n",
    "- The model learns to predict masked gene expression values\n",
    "- This can be used for imputation, denoising, and generating synthetic cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "We'll create synthetic single-cell RNA-seq data with cell type-specific expression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedd.data import create_synthetic_rnaseq, RNASeqDataset, train_val_split\n",
    "\n",
    "# Create synthetic data\n",
    "expression, cell_types, gene_names = create_synthetic_rnaseq(\n",
    "    num_cells=2000,\n",
    "    num_genes=200,\n",
    "    num_cell_types=5,\n",
    "    sparsity=0.7,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f'Expression shape: {expression.shape}')\n",
    "print(f'Cell types: {np.unique(cell_types)}')\n",
    "print(f'Sparsity: {(expression == 0).mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize expression distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Raw expression distribution (non-zero values)\n",
    "non_zero = expression[expression > 0]\n",
    "axes[0].hist(non_zero.flatten(), bins=50, alpha=0.7)\n",
    "axes[0].set_xlabel('Expression')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Raw Expression Distribution (non-zero)')\n",
    "\n",
    "# Log expression\n",
    "axes[1].hist(np.log1p(non_zero.flatten()), bins=50, alpha=0.7)\n",
    "axes[1].set_xlabel('log(1 + Expression)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Log Expression Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Dataset and DataLoaders\n",
    "\n",
    "The `RNASeqDataset` automatically discretizes continuous expression values into bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with discretization\n",
    "NUM_BINS = 100  # Number of expression bins\n",
    "\n",
    "dataset = RNASeqDataset(\n",
    "    expression=expression,\n",
    "    gene_names=gene_names,\n",
    "    cell_labels=cell_types,\n",
    "    num_bins=NUM_BINS,\n",
    "    discretization_method='log_uniform'\n",
    ")\n",
    "\n",
    "print(f'Dataset size: {len(dataset)}')\n",
    "print(f'Number of genes: {dataset.num_genes}')\n",
    "print(f'Number of bins: {dataset.num_bins}')\n",
    "\n",
    "# Split into train/val\n",
    "train_dataset, val_dataset = train_val_split(dataset, val_fraction=0.1, seed=42)\n",
    "print(f'Train size: {len(train_dataset)}, Val size: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize discretized expression\n",
    "sample = dataset[0]\n",
    "print(f'Sample shape: {sample.shape}')\n",
    "print(f'Discretized values range: [{sample.min()}, {sample.max()}]')\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(sample.numpy(), bins=NUM_BINS, alpha=0.7)\n",
    "plt.xlabel('Bin Index')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Discretized Expression for One Cell')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f'Number of training batches: {len(train_loader)}')\n",
    "print(f'Number of validation batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create SEDD Model\n",
    "\n",
    "We'll use a small transformer model suitable for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedd.model import SEDDTransformerSmall\n",
    "from sedd.graph import AbsorbingGraph\n",
    "from sedd.noise import LogLinearNoise\n",
    "\n",
    "# Create model components\n",
    "NUM_GENES = dataset.num_genes\n",
    "VOCAB_SIZE = NUM_BINS + 1  # +1 for mask token\n",
    "\n",
    "model = SEDDTransformerSmall(\n",
    "    num_genes=NUM_GENES,\n",
    "    num_bins=NUM_BINS,\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "graph = AbsorbingGraph(num_states=VOCAB_SIZE)\n",
    "noise = LogLinearNoise(eps=1e-3)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameters: {num_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model\n",
    "\n",
    "We'll train for a few epochs to demonstrate the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedd.trainer import SEDDTrainer\n",
    "\n",
    "# Create trainer\n",
    "trainer = SEDDTrainer(\n",
    "    model=model,\n",
    "    graph=graph,\n",
    "    noise=noise,\n",
    "    device=device,\n",
    "    gradient_clip=1.0\n",
    ")\n",
    "\n",
    "# Train for a few epochs (use more epochs for better results)\n",
    "NUM_EPOCHS = 20\n",
    "MASK_RATIO = 0.15  # Fraction of genes to mask during training\n",
    "\n",
    "history = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    mask_ratio=MASK_RATIO,\n",
    "    log_interval=50,\n",
    "    val_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "ax.plot(history['train_loss'], label='Train Loss')\n",
    "ax.plot(history['val_loss'], label='Val Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Masked Prediction (Imputation)\n",
    "\n",
    "Now we'll test the model's ability to predict masked gene expression values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedd.sampling import impute_masked, EulerSampler\n",
    "\n",
    "# Get a batch of test samples\n",
    "test_batch = next(iter(val_loader)).to(device)\n",
    "print(f'Test batch shape: {test_batch.shape}')\n",
    "\n",
    "# Create random mask (mask 20% of genes)\n",
    "mask_ratio = 0.2\n",
    "mask = torch.rand_like(test_batch.float()) < mask_ratio\n",
    "\n",
    "print(f'Masked positions: {mask.sum().item()} / {mask.numel()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute masked values\n",
    "imputed = impute_masked(\n",
    "    model=model,\n",
    "    graph=graph,\n",
    "    noise=noise,\n",
    "    x=test_batch,\n",
    "    mask=mask,\n",
    "    sampler='euler',\n",
    "    num_steps=50,\n",
    "    temperature=1.0,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate imputation accuracy\n",
    "# Get original values at masked positions\n",
    "original_masked = test_batch[mask]\n",
    "predicted_masked = imputed[mask]\n",
    "\n",
    "# Calculate accuracy (exact match)\n",
    "accuracy = (original_masked == predicted_masked).float().mean().item()\n",
    "print(f'Exact match accuracy: {accuracy:.2%}')\n",
    "\n",
    "# Calculate mean absolute error in bin space\n",
    "mae = (original_masked - predicted_masked).abs().float().mean().item()\n",
    "print(f'Mean Absolute Error (bins): {mae:.2f}')\n",
    "\n",
    "# Calculate within-k accuracy\n",
    "for k in [1, 3, 5, 10]:\n",
    "    within_k = ((original_masked - predicted_masked).abs() <= k).float().mean().item()\n",
    "    print(f'Within {k} bins: {within_k:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs ground truth\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(\n",
    "    original_masked.cpu().numpy(),\n",
    "    predicted_masked.cpu().numpy(),\n",
    "    alpha=0.3,\n",
    "    s=10\n",
    ")\n",
    "axes[0].plot([0, NUM_BINS], [0, NUM_BINS], 'r--', label='Perfect prediction')\n",
    "axes[0].set_xlabel('True Bin')\n",
    "axes[0].set_ylabel('Predicted Bin')\n",
    "axes[0].set_title('Imputation: Predicted vs True')\n",
    "axes[0].legend()\n",
    "\n",
    "# Error distribution\n",
    "errors = (predicted_masked - original_masked).cpu().numpy()\n",
    "axes[1].hist(errors, bins=50, alpha=0.7)\n",
    "axes[1].axvline(0, color='r', linestyle='--', label='Zero error')\n",
    "axes[1].set_xlabel('Prediction Error (bins)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Prediction Error Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Single Cell Imputation\n",
    "\n",
    "Let's look at imputation for a single cell in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single cell example\n",
    "cell_idx = 0\n",
    "original_cell = test_batch[cell_idx].cpu().numpy()\n",
    "imputed_cell = imputed[cell_idx].cpu().numpy()\n",
    "cell_mask = mask[cell_idx].cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 6))\n",
    "\n",
    "# Plot expression profiles\n",
    "gene_indices = np.arange(len(original_cell))\n",
    "\n",
    "# Original\n",
    "axes[0].bar(gene_indices, original_cell, alpha=0.7, label='Original', width=1.0)\n",
    "axes[0].scatter(gene_indices[cell_mask], original_cell[cell_mask], \n",
    "                c='red', s=20, zorder=5, label='Masked positions')\n",
    "axes[0].set_xlabel('Gene Index')\n",
    "axes[0].set_ylabel('Expression Bin')\n",
    "axes[0].set_title('Original Expression (masked positions highlighted)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Imputed\n",
    "axes[1].bar(gene_indices, imputed_cell, alpha=0.7, label='Imputed', width=1.0)\n",
    "axes[1].scatter(gene_indices[cell_mask], imputed_cell[cell_mask], \n",
    "                c='green', s=20, zorder=5, label='Imputed positions')\n",
    "axes[1].set_xlabel('Gene Index')\n",
    "axes[1].set_ylabel('Expression Bin')\n",
    "axes[1].set_title('Imputed Expression')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate cell-specific accuracy\n",
    "cell_acc = (original_cell[cell_mask] == imputed_cell[cell_mask]).mean()\n",
    "cell_mae = np.abs(original_cell[cell_mask] - imputed_cell[cell_mask]).mean()\n",
    "print(f'Cell {cell_idx} - Accuracy: {cell_acc:.2%}, MAE: {cell_mae:.2f} bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate New Cells (De Novo Generation)\n",
    "\n",
    "We can also use SEDD to generate entirely new cell expression profiles by starting from all masked tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedd.sampling import EulerSampler\n",
    "\n",
    "# Create sampler for generation\n",
    "sampler = EulerSampler(\n",
    "    model=model,\n",
    "    graph=graph,\n",
    "    noise=noise,\n",
    "    num_steps=100,\n",
    "    device=device,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "# Generate from all-masked starting point\n",
    "num_generate = 5\n",
    "x_init = graph.sample_limiting((num_generate, NUM_GENES), device)\n",
    "\n",
    "print(f'Initial (all masked): {x_init}')\n",
    "\n",
    "generated = sampler.sample(x_init, show_progress=True)\n",
    "print(f'Generated shape: {generated.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare generated vs real cells\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Plot real cells\n",
    "for i in range(3):\n",
    "    axes[0, i].bar(range(NUM_GENES), test_batch[i].cpu().numpy(), alpha=0.7, width=1.0)\n",
    "    axes[0, i].set_xlabel('Gene')\n",
    "    axes[0, i].set_ylabel('Bin')\n",
    "    axes[0, i].set_title(f'Real Cell {i+1}')\n",
    "\n",
    "# Plot generated cells\n",
    "for i in range(3):\n",
    "    axes[1, i].bar(range(NUM_GENES), generated[i].cpu().numpy(), alpha=0.7, width=1.0)\n",
    "    axes[1, i].set_xlabel('Gene')\n",
    "    axes[1, i].set_ylabel('Bin')\n",
    "    axes[1, i].set_title(f'Generated Cell {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare statistics\n",
    "real_mean = test_batch.float().mean(dim=0).cpu().numpy()\n",
    "gen_mean = generated.float().mean(dim=0).cpu().numpy()\n",
    "\n",
    "real_std = test_batch.float().std(dim=0).cpu().numpy()\n",
    "gen_std = generated.float().std(dim=0).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(real_mean, gen_mean, alpha=0.5)\n",
    "axes[0].plot([0, max(real_mean.max(), gen_mean.max())], \n",
    "             [0, max(real_mean.max(), gen_mean.max())], 'r--')\n",
    "axes[0].set_xlabel('Real Mean Expression')\n",
    "axes[0].set_ylabel('Generated Mean Expression')\n",
    "axes[0].set_title('Mean Expression per Gene')\n",
    "\n",
    "axes[1].scatter(real_std, gen_std, alpha=0.5)\n",
    "axes[1].plot([0, max(real_std.max(), gen_std.max())], \n",
    "             [0, max(real_std.max(), gen_std.max())], 'r--')\n",
    "axes[1].set_xlabel('Real Std Expression')\n",
    "axes[1].set_ylabel('Generated Std Expression')\n",
    "axes[1].set_title('Expression Variance per Gene')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "mean_corr = np.corrcoef(real_mean, gen_mean)[0, 1]\n",
    "print(f'Mean expression correlation: {mean_corr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save and Load Model\n",
    "\n",
    "Finally, let's save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "trainer.save_checkpoint('checkpoints/demo_model.pt')\n",
    "print('Model saved to checkpoints/demo_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (demonstration)\n",
    "# Create a new model instance\n",
    "new_model = SEDDTransformerSmall(\n",
    "    num_genes=NUM_GENES,\n",
    "    num_bins=NUM_BINS,\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "new_trainer = SEDDTrainer(\n",
    "    model=new_model,\n",
    "    graph=graph,\n",
    "    noise=noise,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "new_trainer.load_checkpoint('checkpoints/demo_model.pt')\n",
    "print(f'Model loaded! Trained for {new_trainer.epoch + 1} epochs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. **Data preparation**: Creating and discretizing single-cell RNA-seq data\n",
    "2. **Model setup**: Creating a SEDD transformer with absorbing diffusion\n",
    "3. **Training**: Training the model to predict masked gene expression\n",
    "4. **Imputation**: Using the trained model to fill in missing expression values\n",
    "5. **Generation**: Generating novel cell expression profiles from scratch\n",
    "\n",
    "### Tips for Better Results\n",
    "\n",
    "- **Train longer**: Use more epochs (100+) for better imputation accuracy\n",
    "- **Larger model**: Use SEDDTransformerMedium or SEDDTransformerLarge for larger datasets\n",
    "- **More bins**: Increase num_bins for finer expression resolution\n",
    "- **Tune mask ratio**: Experiment with different training mask ratios (0.1-0.3)\n",
    "- **Lower temperature**: Use temperature < 1.0 for more confident predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
