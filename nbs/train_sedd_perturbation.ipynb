{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SEDD Model on Perturbation Data\n",
    "\n",
    "This notebook demonstrates how to train a SEDD (Score-Entropy Discrete Diffusion) model on perturbation-seq data.\n",
    "\n",
    "**Dataset format:**\n",
    "- h5ad file with gene expression matrix (cells × genes)\n",
    "- Perturbation column (e.g., 'gene', 'perturbation') with control cells marked as 'non-targeting'\n",
    "- Expression values will be discretized into bins\n",
    "\n",
    "**Model task:**\n",
    "Given a control cell state and a perturbation label, predict the perturbed cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "Load your h5ad file and configure the dataset parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURE YOUR DATASET HERE ==========\n",
    "\n",
    "# Path to your h5ad file\n",
    "DATA_PATH = \"path/to/your/dataset.h5ad\"  # CHANGE THIS\n",
    "\n",
    "# Column name containing perturbation labels\n",
    "PERT_COL = \"gene\"  # or \"perturbation\", \"treatment\", etc.\n",
    "\n",
    "# Name of control/baseline perturbation\n",
    "CONTROL_NAME = \"non-targeting\"  # or \"control\", \"mock\", etc.\n",
    "\n",
    "# Expression discretization parameters\n",
    "NUM_BINS = 100  # Number of bins for discretizing gene expression\n",
    "USE_LOG_TRANSFORM = True  # Apply log1p transform before discretization\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "MASK_RATIO = 0.15  # Fraction of genes to mask during training\n",
    "\n",
    "# Model size (\"small\", \"medium\", or \"large\")\n",
    "MODEL_SIZE = \"small\"\n",
    "\n",
    "# =================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Data path: {DATA_PATH}\")\n",
    "print(f\"  Perturbation column: {PERT_COL}\")\n",
    "print(f\"  Control name: {CONTROL_NAME}\")\n",
    "print(f\"  Number of bins: {NUM_BINS}\")\n",
    "print(f\"  Log transform: {USE_LOG_TRANSFORM}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Model size: {MODEL_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the h5ad file\n",
    "print(f\"Loading data from {DATA_PATH}...\")\n",
    "adata = sc.read_h5ad(DATA_PATH)\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Shape: {adata.shape} (cells × genes)\")\n",
    "print(f\"  Number of cells: {adata.n_obs:,}\")\n",
    "print(f\"  Number of genes: {adata.n_vars:,}\")\n",
    "\n",
    "# Check perturbation column\n",
    "if PERT_COL not in adata.obs.columns:\n",
    "    raise ValueError(f\"Perturbation column '{PERT_COL}' not found in data. Available columns: {list(adata.obs.columns)}\")\n",
    "\n",
    "# Count perturbations\n",
    "pert_counts = adata.obs[PERT_COL].value_counts()\n",
    "print(f\"\\nPerturbation info:\")\n",
    "print(f\"  Total unique perturbations: {len(pert_counts)}\")\n",
    "print(f\"  Control cells ({CONTROL_NAME}): {pert_counts.get(CONTROL_NAME, 0):,}\")\n",
    "print(f\"  Perturbed cells: {adata.n_obs - pert_counts.get(CONTROL_NAME, 0):,}\")\n",
    "\n",
    "print(f\"\\nTop 10 perturbations:\")\n",
    "for pert, count in pert_counts.head(10).items():\n",
    "    print(f\"  {pert}: {count:,} cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract expression matrix\n",
    "# Handle sparse matrices\n",
    "if hasattr(adata.X, 'toarray'):\n",
    "    expression = adata.X.toarray()\n",
    "else:\n",
    "    expression = adata.X\n",
    "\n",
    "print(f\"Expression matrix shape: {expression.shape}\")\n",
    "print(f\"Expression range: {expression.min():.2f} to {expression.max():.2f}\")\n",
    "print(f\"Sparsity (zeros): {(expression == 0).sum() / expression.size:.1%}\")\n",
    "\n",
    "# Visualize expression distribution\n",
    "non_zero_expr = expression[expression > 0]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(non_zero_expr.flatten(), bins=100, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Expression Value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Expression Distribution (non-zero values)')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nExpression statistics (non-zero values):\")\n",
    "print(f\"  Mean: {non_zero_expr.mean():.2f}\")\n",
    "print(f\"  Median: {np.median(non_zero_expr):.2f}\")\n",
    "print(f\"  Std: {non_zero_expr.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discretize Expression Values\n",
    "\n",
    "SEDD is a discrete diffusion model, so we need to convert continuous expression values into discrete bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize expression values\n",
    "print(\"Discretizing expression values...\")\n",
    "\n",
    "# Optional: Apply log transform\n",
    "if USE_LOG_TRANSFORM:\n",
    "    expression_processed = np.log1p(expression)\n",
    "    print(\"Applied log1p transformation\")\n",
    "else:\n",
    "    expression_processed = expression.copy()\n",
    "\n",
    "# Discretize into bins\n",
    "# Method: Quantile-based binning for better distribution\n",
    "def discretize_expression(expr, num_bins=100):\n",
    "    \"\"\"\n",
    "    Discretize expression values into bins.\n",
    "    Uses quantile-based binning for uniform distribution across bins.\n",
    "    \"\"\"\n",
    "    # Flatten non-zero values for computing quantiles\n",
    "    non_zero_mask = expr > 0\n",
    "    non_zero_vals = expr[non_zero_mask]\n",
    "    \n",
    "    # Compute quantile boundaries\n",
    "    quantiles = np.linspace(0, 100, num_bins + 1)\n",
    "    boundaries = np.percentile(non_zero_vals, quantiles)\n",
    "    boundaries[0] = 0  # Ensure zero is included\n",
    "    boundaries[-1] = expr.max() + 1e-10  # Ensure max value is included\n",
    "    \n",
    "    # Digitize\n",
    "    discretized = np.digitize(expr, boundaries) - 1\n",
    "    discretized = np.clip(discretized, 0, num_bins - 1)\n",
    "    \n",
    "    return discretized.astype(np.int64)\n",
    "\n",
    "expression_discrete = discretize_expression(expression_processed, NUM_BINS)\n",
    "\n",
    "print(f\"Discretized expression shape: {expression_discrete.shape}\")\n",
    "print(f\"Discrete values range: {expression_discrete.min()} to {expression_discrete.max()}\")\n",
    "print(f\"Number of unique values: {len(np.unique(expression_discrete))}\")\n",
    "\n",
    "# Visualize discretized distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(expression_discrete.flatten(), bins=NUM_BINS, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Bin Index')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Discretized Expression Distribution')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedd.data import PerturbSeqDataset, train_val_split\n",
    "\n",
    "# Extract perturbation labels\n",
    "pert_labels = adata.obs[PERT_COL].values\n",
    "gene_names = adata.var_names.tolist()\n",
    "\n",
    "# Create dataset\n",
    "print(\"Creating PerturbSeqDataset...\")\n",
    "dataset = PerturbSeqDataset(\n",
    "    expression=expression_discrete,\n",
    "    pert_labels=pert_labels,\n",
    "    gene_names=gene_names,\n",
    "    num_bins=NUM_BINS,\n",
    "    control_pert_name=CONTROL_NAME,\n",
    ")\n",
    "\n",
    "# Split into train/val\n",
    "train_dataset, val_dataset = train_val_split(dataset, val_fraction=0.1, seed=42)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset):,} samples\")\n",
    "print(f\"  Val: {len(val_dataset):,} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader info:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Test the dataloader\n",
    "control, pert_label, perturbed = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  Control shape: {control.shape}\")\n",
    "print(f\"  Pert label shape: {pert_label.shape}\")\n",
    "print(f\"  Perturbed shape: {perturbed.shape}\")\n",
    "\n",
    "# Store dataset parameters\n",
    "NUM_GENES = dataset.num_genes\n",
    "NUM_PERTURBATIONS = dataset.num_perturbations\n",
    "VOCAB_SIZE = NUM_BINS + 1  # +1 for mask token\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  NUM_GENES: {NUM_GENES}\")\n",
    "print(f\"  NUM_BINS: {NUM_BINS}\")\n",
    "print(f\"  NUM_PERTURBATIONS: {NUM_PERTURBATIONS}\")\n",
    "print(f\"  VOCAB_SIZE: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create SEDD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedd.model import (\n",
    "    SEDDPerturbationTransformerSmall,\n",
    "    SEDDPerturbationTransformerMedium,\n",
    "    SEDDPerturbationTransformerLarge,\n",
    ")\n",
    "from sedd.graph import AbsorbingGraph\n",
    "from sedd.noise import LogLinearNoise\n",
    "\n",
    "# Select model size\n",
    "if MODEL_SIZE == \"small\":\n",
    "    ModelClass = SEDDPerturbationTransformerSmall\n",
    "    model_config = {\"hidden_dim\": 128, \"num_layers\": 4, \"num_heads\": 4}\n",
    "elif MODEL_SIZE == \"medium\":\n",
    "    ModelClass = SEDDPerturbationTransformerMedium\n",
    "    model_config = {\"hidden_dim\": 256, \"num_layers\": 6, \"num_heads\": 8}\n",
    "elif MODEL_SIZE == \"large\":\n",
    "    ModelClass = SEDDPerturbationTransformerLarge\n",
    "    model_config = {\"hidden_dim\": 512, \"num_layers\": 8, \"num_heads\": 8}\n",
    "else:\n",
    "    raise ValueError(f\"Invalid model size: {MODEL_SIZE}. Choose 'small', 'medium', or 'large'.\")\n",
    "\n",
    "print(f\"Creating {MODEL_SIZE} SEDD model...\")\n",
    "print(f\"  Config: {model_config}\")\n",
    "\n",
    "# Create model\n",
    "model = ModelClass(\n",
    "    num_genes=NUM_GENES,\n",
    "    num_bins=NUM_BINS,\n",
    "    num_perturbations=NUM_PERTURBATIONS,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "# Create diffusion graph (absorbing state = mask token)\n",
    "graph = AbsorbingGraph(num_states=VOCAB_SIZE)\n",
    "\n",
    "# Create noise schedule\n",
    "noise = LogLinearNoise(eps=1e-3)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel info:\")\n",
    "print(f\"  Total parameters: {num_params:,}\")\n",
    "print(f\"  Trainable parameters: {num_trainable:,}\")\n",
    "print(f\"  Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedd.trainer import PerturbationTrainer\n",
    "\n",
    "# Create trainer\n",
    "trainer = PerturbationTrainer(\n",
    "    model=model,\n",
    "    graph=graph,\n",
    "    noise=noise,\n",
    "    device=device,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gradient_clip=1.0,\n",
    ")\n",
    "\n",
    "print(f\"Trainer created. Starting training for {NUM_EPOCHS} epochs...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    mask_ratio=MASK_RATIO,\n",
    "    log_interval=1,\n",
    "    val_interval=1,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final val loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Best val loss: {trainer.best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "ax.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training Progress', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Get a test batch from validation set\n",
    "test_control, test_pert_labels, test_perturbed = next(iter(val_loader))\n",
    "test_control = test_control.to(device)\n",
    "test_pert_labels = test_pert_labels.to(device)\n",
    "test_perturbed = test_perturbed.to(device)\n",
    "\n",
    "print(f\"Test batch:\")\n",
    "print(f\"  Controls: {test_control.shape}\")\n",
    "print(f\"  Pert labels: {test_pert_labels.shape}\")\n",
    "print(f\"  True perturbed: {test_perturbed.shape}\")\n",
    "\n",
    "# Simple inference: forward pass with low noise\n",
    "# For production, you'd use the full sampling procedure\n",
    "with torch.no_grad():\n",
    "    batch_size = test_control.shape[0]\n",
    "    \n",
    "    # Start from all masked\n",
    "    x_init = torch.full((batch_size, NUM_GENES), NUM_BINS, device=device, dtype=torch.long)\n",
    "    \n",
    "    # Forward pass with low noise\n",
    "    sigma = torch.ones(batch_size, device=device) * 0.01\n",
    "    logits = model(x_init, sigma, test_pert_labels)\n",
    "    predicted = logits.argmax(dim=-1)\n",
    "\n",
    "print(f\"\\nPredicted perturbed cells: {predicted.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "with torch.no_grad():\n",
    "    # Exact match accuracy\n",
    "    accuracy = (predicted == test_perturbed).float().mean().item()\n",
    "    print(f\"Evaluation metrics:\")\n",
    "    print(f\"  Exact match accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Mean absolute error\n",
    "    mae = (predicted - test_perturbed).abs().float().mean().item()\n",
    "    print(f\"  Mean Absolute Error: {mae:.2f} bins\")\n",
    "    \n",
    "    # Within-k accuracy\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        within_k = ((predicted - test_perturbed).abs() <= k).float().mean().item()\n",
    "        print(f\"  Within {k} bins: {within_k:.2%}\")\n",
    "    \n",
    "    # Per-gene correlation\n",
    "    correlations = []\n",
    "    for gene_idx in range(NUM_GENES):\n",
    "        true_gene = test_perturbed[:, gene_idx].float().cpu().numpy()\n",
    "        pred_gene = predicted[:, gene_idx].float().cpu().numpy()\n",
    "        if true_gene.std() > 0 and pred_gene.std() > 0:\n",
    "            corr = np.corrcoef(true_gene, pred_gene)[0, 1]\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append(corr)\n",
    "    \n",
    "    if correlations:\n",
    "        print(f\"  Average per-gene correlation: {np.mean(correlations):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs ground truth\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "true_flat = test_perturbed.cpu().numpy().flatten()\n",
    "pred_flat = predicted.cpu().numpy().flatten()\n",
    "max_val = NUM_BINS\n",
    "\n",
    "# Sample points for visualization (to avoid overcrowding)\n",
    "sample_size = min(10000, len(true_flat))\n",
    "sample_idx = np.random.choice(len(true_flat), sample_size, replace=False)\n",
    "true_sample = true_flat[sample_idx]\n",
    "pred_sample = pred_flat[sample_idx]\n",
    "\n",
    "axes[0].scatter(true_sample, pred_sample, alpha=0.2, s=5)\n",
    "axes[0].plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('True Expression (bin)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Expression (bin)', fontsize=12)\n",
    "axes[0].set_title('Predicted vs True Expression', fontsize=13)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "errors = pred_flat - true_flat\n",
    "axes[1].hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(0, color='r', linestyle='--', linewidth=2, label='Zero error')\n",
    "axes[1].set_xlabel('Prediction Error (bins)', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Prediction Error Distribution', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a single cell prediction\n",
    "cell_idx = 0\n",
    "\n",
    "control_cell = test_control[cell_idx].cpu().numpy()\n",
    "true_perturbed = test_perturbed[cell_idx].cpu().numpy()\n",
    "pred_perturbed = predicted[cell_idx].cpu().numpy()\n",
    "pert_label = test_pert_labels[cell_idx].item()\n",
    "\n",
    "# Get perturbation name if available\n",
    "if dataset.idx_to_pert is not None:\n",
    "    pert_name = dataset.idx_to_pert[pert_label]\n",
    "else:\n",
    "    pert_name = f\"Perturbation {pert_label}\"\n",
    "\n",
    "# Visualize subset of genes\n",
    "n_genes_viz = min(200, NUM_GENES)\n",
    "gene_indices = np.arange(n_genes_viz)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "# Control\n",
    "axes[0].bar(gene_indices, control_cell[:n_genes_viz], alpha=0.7, width=1.0, color='blue')\n",
    "axes[0].set_ylabel('Expression (bin)', fontsize=11)\n",
    "axes[0].set_title(f'Control Cell Expression (first {n_genes_viz} genes)', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# True perturbed\n",
    "axes[1].bar(gene_indices, true_perturbed[:n_genes_viz], alpha=0.7, width=1.0, color='red')\n",
    "axes[1].set_ylabel('Expression (bin)', fontsize=11)\n",
    "axes[1].set_title(f'True Perturbed Cell - {pert_name}', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Predicted\n",
    "axes[2].bar(gene_indices, pred_perturbed[:n_genes_viz], alpha=0.7, width=1.0, color='green')\n",
    "axes[2].set_ylabel('Expression (bin)', fontsize=11)\n",
    "axes[2].set_title(f'Predicted Perturbed Cell', fontsize=12)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Error\n",
    "error = (pred_perturbed - true_perturbed)[:n_genes_viz]\n",
    "colors = ['green' if abs(e) <= 5 else 'orange' if abs(e) <= 10 else 'red' for e in error]\n",
    "axes[3].bar(gene_indices, error, alpha=0.7, width=1.0, color=colors)\n",
    "axes[3].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[3].set_xlabel('Gene Index', fontsize=11)\n",
    "axes[3].set_ylabel('Prediction Error', fontsize=11)\n",
    "axes[3].set_title('Prediction Error (Green: ≤5, Orange: ≤10, Red: >10 bins)', fontsize=12)\n",
    "axes[3].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell metrics\n",
    "cell_acc = (true_perturbed == pred_perturbed).mean()\n",
    "cell_mae = np.abs(pred_perturbed - true_perturbed).mean()\n",
    "print(f'\\nCell {cell_idx} ({pert_name}) metrics:')\n",
    "print(f'  Accuracy: {cell_acc:.2%}')\n",
    "print(f'  MAE: {cell_mae:.2f} bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "checkpoint_path = f'../checkpoints/sedd_perturbation_{MODEL_SIZE}.pt'\n",
    "trainer.save_checkpoint(checkpoint_path)\n",
    "print(f'Model saved to {checkpoint_path}')\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "history_path = f'../checkpoints/sedd_perturbation_{MODEL_SIZE}_history.pkl'\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "print(f'Training history saved to {history_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Model (Optional)\n",
    "\n",
    "To load a saved model for inference or continued training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a saved model\n",
    "# Uncomment to use\n",
    "\n",
    "# new_model = ModelClass(\n",
    "#     num_genes=NUM_GENES,\n",
    "#     num_bins=NUM_BINS,\n",
    "#     num_perturbations=NUM_PERTURBATIONS,\n",
    "#     dropout=0.1,\n",
    "# ).to(device)\n",
    "\n",
    "# new_trainer = PerturbationTrainer(\n",
    "#     model=new_model,\n",
    "#     graph=graph,\n",
    "#     noise=noise,\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "# new_trainer.load_checkpoint(checkpoint_path)\n",
    "# print(f'Model loaded from {checkpoint_path}')\n",
    "# print(f'Trained for {new_trainer.epoch + 1} epochs')\n",
    "# print(f'Best validation loss: {new_trainer.best_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
